apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: llm-storage
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  resources:
    requests:
      storage: 5Gi
---
kind: Deployment
apiVersion: apps/v1
metadata:
  name: llm-server
spec:
  replicas: 1
  selector:
    matchLabels:
      app: llm-server
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: llm-server
    spec:
      volumes:
        - name: llm-storage
          persistentVolumeClaim:
            claimName: llm-storage
      containers:
        - name: container
          image: 'quay.io/wcaban/ollama:latest'
          ports:
            - containerPort: 11434
              protocol: TCP
          resources: {}
          volumeMounts:
            - name: llm-storage
              mountPath: /.ollama
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          imagePullPolicy: Always
          lifecycle:
            postStart:
              exec:
                command: ["/bin/sh", "-c", "ollama pull qwen2.5:0.5b-instruct"]
      restartPolicy: Always


---
apiVersion: v1
kind: Service
metadata:
  name: llm
spec:
  ports:
  - port: 8000
    protocol: TCP
    targetPort: 11434
  selector:
    app: llm-server
  type: ClusterIP